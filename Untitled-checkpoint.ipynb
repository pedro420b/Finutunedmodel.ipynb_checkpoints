{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618b3a3d-2785-4c86-a6f3-d7056fe19d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in c:\\users\\badrx\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: torch==2.4.1 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torchaudio) (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torch==2.4.1->torchaudio) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torch==2.4.1->torchaudio) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torch==2.4.1->torchaudio) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torch==2.4.1->torchaudio) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torch==2.4.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torch==2.4.1->torchaudio) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from torch==2.4.1->torchaudio) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from jinja2->torch==2.4.1->torchaudio) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/992.0 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/992.0 kB 325.1 kB/s eta 0:00:03\n",
      "   -- ------------------------------------ 71.7/992.0 kB 491.5 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 266.2/992.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  983.0/992.0 kB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 992.0/992.0 kB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchaudio sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d51200-e0ae-4678-9bc1-858ff46653d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915e2d0dcc3644c39827df226d02b2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff792930607e41018eb053f8a5dae5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/124M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-mustc-en-de-st and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9520cc504d4b17bb1d509aca5babf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394ebbcad93b4d2c9052c27ca91d42f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/s2t-small-mustc-en-de-st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8d6fe3-6879-401a-b0a0-7705a4646196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 131.3 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 30.7/44.4 kB 163.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 199.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\badrx\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.9 MB 2.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/9.9 MB 4.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.5/9.9 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 17.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.0/9.9 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.1/9.9 MB 19.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.2/9.9 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.9 MB 20.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.9 MB 21.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 19.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 436.4/436.4 kB 28.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.3/286.3 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.3/2.3 MB 40.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.3 MB 30.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.3 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.3 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.3/2.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.1 safetensors-0.4.5 tokenizers-0.20.0 transformers-4.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e981a6b-e28b-419c-b915-887d14afb079",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Speech2TextForConditionalGeneration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Speech2TextForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/s2t-small-librispeech-asr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m processor \u001b[38;5;241m=\u001b[39m Speech2TextProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/s2t-small-librispeech-asr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Speech2TextForConditionalGeneration' is not defined"
     ]
    }
   ],
   "source": [
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c0621a-9b41-44ae-a2f7-a1a488f3b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019a8ec9-eda0-4117-a703-3176ba651e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796dec944bb54d92a53226db36d2c0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f75c06ca28a4aa9ab38558949ea7f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/118M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54a66fb397f40b883c1abc822906060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf37bcd8bf4429490e7f63dac1a2141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31be2ea195c4419990d7263b2d2efcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/456 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd62abcf7775433493e3f8d601264a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/235k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f109f3ae7a8e429eb745581b5b86e1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/417k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebee8e91f874064a4b7334cb47e25dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badrx\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6669c5f3-4dfe-4e08-bf69-87a87e944451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the decoder: <class 'transformers.models.deprecated.speech_to_text_2.modeling_speech_to_text_2.Speech2Text2ForCausalLM'> is overwritten by shared decoder config: Speech2Text2Config {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"Speech2TextForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"conv_channels\": 1024,\n",
      "  \"conv_kernel_sizes\": [\n",
      "    5,\n",
      "    5\n",
      "  ],\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 7,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"input_channels\": 1,\n",
      "  \"input_feat_per_channel\": 80,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_source_positions\": 6000,\n",
      "  \"max_target_positions\": 1024,\n",
      "  \"model_type\": \"speech_to_text_2\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_conv_layers\": 2,\n",
      "  \"num_hidden_layers\": 7,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "             # Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/s2t-wav2vec2-large-en-de\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1325e212-1a5c-421b-bd7e-a46f8a0bc039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the decoder: <class 'transformers.models.deprecated.speech_to_text_2.modeling_speech_to_text_2.Speech2Text2ForCausalLM'> is overwritten by shared decoder config: Speech2Text2Config {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"Speech2TextForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"conv_channels\": 1024,\n",
      "  \"conv_kernel_sizes\": [\n",
      "    5,\n",
      "    5\n",
      "  ],\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 7,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"input_channels\": 1,\n",
      "  \"input_feat_per_channel\": 80,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_source_positions\": 6000,\n",
      "  \"max_target_positions\": 1024,\n",
      "  \"model_type\": \"speech_to_text_2\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_conv_layers\": 2,\n",
      "  \"num_hidden_layers\": 7,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c914a618-5889-417a-944f-e947f4dcdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ba04253-6c5e-430c-a9eb-ea990cacf35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badrx\\anaconda3\\already_finetuned\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09ad7fb1-e659-4564-8486-5ef0fe9c935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_file(filename, search_path):\n",
    "    result = []\n",
    "    # Rekursiv alle Dateien und Verzeichnisse im Suchpfad durchsuchen\n",
    "    for root, dirs, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            # Wenn die Datei gefunden wurde, den kompletten Pfad speichern\n",
    "            result.append(os.path.join(root, filename))\n",
    "\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468a15cf-04d3-4618-a344-9473bf44f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"deepspeech-0.9.3-models.pbmm\" \n",
    "search_path = \"C:\\\\\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f784ab0-b94c-4326-81c6-b13986b56d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_file(filename, search_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e816f39b-f85f-4f00-9a59-e93d15c9fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Datei nicht gefunden.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "found_files = find_file(filename, search_path)\n",
    "\n",
    "if found_files:\n",
    "    print(f\"Datei(en) gefunden: {found_files}\")\n",
    "else:\n",
    "    print(\"Datei nicht gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c6cf70c-4a69-4298-b88b-8a050cd3e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73c12fdb-7628-4e23-ad22-dbaab82a4b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91264e521e6840e9b708abc4d451cb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf1e019c8cd43e4a70f84b76c57f316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60 and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d359d1bce24ea5b3e4e0e4adef4c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb759722ae64217847b0b83614c0ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17d35ee05a84095921605c83421b94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa613ceeaf346008ea3835d785c193b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-large-960h-lv60\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "360fff2f-2316-484e-b6cc-b1ba57cef975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lade die lokale Audiodatei\n",
    "audio_file = \"C:/Users/badrx/anaconda3/already_finetuned/harvard.wav\"\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1950e5a3-1832-4593-b977-6e87490c2f90",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AutomaticSpeechRecognitionPipeline._sanitize_parameters() got an unexpected keyword argument 'sampling_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Speech-to-Text Auswertung\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m speech_recognizer(waveform\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy(), sampling_rate\u001b[38;5;241m=\u001b[39msample_rate)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ergebnis anzeigen\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:284\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    223\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    225\u001b[0m ):\n\u001b[0;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1221\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1219\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size\n\u001b[1;32m-> 1221\u001b[0m preprocess_params, forward_params, postprocess_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_parameters(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;66;03m# Fuse __init__ params and __call__ params without modifying the __init__ ones.\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m preprocess_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params}\n",
      "\u001b[1;31mTypeError\u001b[0m: AutomaticSpeechRecognitionPipeline._sanitize_parameters() got an unexpected keyword argument 'sampling_rate'"
     ]
    }
   ],
   "source": [
    "# Speech-to-Text Auswertung\n",
    "result = speech_recognizer(waveform.squeeze().numpy(), sampling_rate=sample_rate)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(result['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37943ce5-8cb9-4279-9dd9-2dc96284b96b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mein name ist anna zakrison am ich bin in stockorm gebolenen schweden aber aufgewachsen im norden am ich hab erstmal biulgistudierten chemisch universit채 in england r und habmmalund marsters und kamtemartes gekriegt hab dann man die plumn gemacht bei max plank und dan pihal und pihadiasomp promosionin n stockom juniversit namlicha gastforschung gearbeitet bei leibnichsinstitut und er haben paar andere posissionen auch gehabt im bisnis  heidolconton da cegino und jetzt arbeite ich mit meiner eigene kommunikationsplatform dkoranasimagin채ri um dasses we wissenschaftskommunikation under bi biologische konsolting und ich finde skepticismus unfasbar wichtig falls wir uns wirklich eine nachhaltige zukunft als menschen w체nschen besonderweise mit dem klimaver채nderungen usw\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the Speech-to-Text pipeline\n",
    "speech_recognizer = pipeline(\"automatic-speech-recognition\",model=\"facebook/wav2vec2-large-xlsr-53-german\")\n",
    "\n",
    "# Specify the path to your local audio file\n",
    "audio_file = \"C:/Users/badrx/anaconda3/already_finetuned/Anna_Zakrisson_DE.ogg.mp3\"  # Update this with your actual file path\n",
    "\n",
    "# Use the pipeline with the audio file path directly\n",
    "result = speech_recognizer(audio_file)\n",
    "\n",
    "# Display the transcription result\n",
    "print(result['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1ae31-e36a-4e5f-9e2e-a6e132026dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
